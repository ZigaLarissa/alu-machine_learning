{"cells":[{"cell_type":"markdown","metadata":{"id":"EAt-K2qgcIou"},"source":["# Optimization Using Gradient Descent: Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"FZYK-0rin5x7"},"source":["In this assignment, you will build a simple linear regression model to predict sales based on TV marketing expenses. You will investigate three different approaches to this problem. You will use `NumPy` and `Scikit-Learn` linear regression models, as well as construct and optimize the sum of squares cost function with gradient descent from scratch.\n","\n","Further you will add additional cells to compare Linear regression and atleast 1 other algorithm"]},{"cell_type":"markdown","metadata":{"id":"Ywl11dna6rPV"},"source":["# Table of Contents\n","\n","- [ 1 - Open the Dataset and State the Problem]\n","  - [ Exercise 1]\n","- [ 2 - Linear Regression in Python with `NumPy` and `Scikit-Learn`]\n","  - [ 2.1 - Linear Regression with `NumPy`]\n","    - [ Exercise 2]\n","  - [ 2.2 - Linear Regression with `Scikit-Learn`]\n","    - [ Exercise 3]\n","    - [ Exercise 4]\n","- [ 3 - Linear Regression using Gradient Descent]\n","  - [ Exercise 5]\n","  - [ Exercise 6]"]},{"cell_type":"markdown","metadata":{"id":"QMoxIfha6rPV"},"source":["## Packages\n","\n","Load the required packages:"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"jaaw4ei_6rPW","tags":["graded"]},"outputs":[],"source":["# A library for programmatic plot generation.\n","# A library for data manipulation and analysis.\n","# LinearRegression from sklearn.\n","\n","from sklearn.linear_model import LinearRegression\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","import seaborn as sns\n"]},{"cell_type":"markdown","metadata":{"id":"_jIkxZQI6rPX"},"source":["Import the unit tests defined for this notebook."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"2BA4_EOR6rPY"},"outputs":[],"source":["import w2_unittest"]},{"cell_type":"markdown","metadata":{"id":"obKIZJlp6rPY"},"source":["<a name='1'></a>\n","## 1 - Open the Dataset and State the Problem"]},{"cell_type":"markdown","metadata":{"id":"68gkhFLw6rPY"},"source":["In this lab, you will build a linear regression model for a simple Kaggle dataset, saved in a file `data/tvmarketing.csv`. The dataset has only two fields: TV marketing expenses (`TV`) and sales amount (`Sales`)."]},{"cell_type":"markdown","metadata":{"id":"htARQfsB6rPZ"},"source":["<a name='ex01'></a>\n","### Exercise 1\n","\n","Use `pandas` function `pd.read_csv` to open the .csv file the from the `path`."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"oIpEVfK56rPZ","tags":["graded"]},"outputs":[],"source":["path = \"data/tvmarketing.csv\"\n","\n","### START CODE HERE ### (~ 1 line of code)\n","adv = pd.read_csv(path)\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"twBM6N2s6rPZ","tags":["graded"]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TV</th>\n","      <th>Sales</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>230.1</td>\n","      <td>22.1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>44.5</td>\n","      <td>10.4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>17.2</td>\n","      <td>9.3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>151.5</td>\n","      <td>18.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>180.8</td>\n","      <td>12.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      TV  Sales\n","0  230.1   22.1\n","1   44.5   10.4\n","2   17.2    9.3\n","3  151.5   18.5\n","4  180.8   12.9"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Print some part of the dataset.\n","adv.head()"]},{"cell_type":"markdown","metadata":{"id":"fQk_r2cK6rPZ"},"source":["##### __Expected Output__\n","\n","```Python\n","\tTV\tSales\n","0\t230.1\t22.1\n","1\t44.5\t10.4\n","2\t17.2\t9.3\n","3\t151.5\t18.5\n","4\t180.8\t12.9\n","```"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"z09PR0n56rPa"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[92m All tests passed\n"]}],"source":["w2_unittest.test_load_data(adv)"]},{"cell_type":"markdown","metadata":{"id":"ih0JPZfU6rPa"},"source":["`pandas` has a function to make plots from the DataFrame fields. By default, matplotlib is used at the backend. Let's use it here:"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"bYemvzOF6rPa","tags":["graded"]},"outputs":[{"data":{"text/plain":["<Axes: xlabel='TV', ylabel='Sales'>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["adv.plot(x='TV', y='Sales', kind='scatter', c='black')"]},{"cell_type":"markdown","metadata":{"id":"8v3CxHDw6rPa"},"source":["You can use this dataset to solve a simple problem with linear regression: given a TV marketing budget, predict sales."]},{"cell_type":"markdown","metadata":{"id":"PAxfJx4K6rPa"},"source":["<a name='2'></a>\n","## 2 - Linear Regression in Python with `NumPy` and `Scikit-Learn`"]},{"cell_type":"markdown","metadata":{"id":"1iDBn7sN6rPa"},"source":["Save the required field of the DataFrame into variables `X` and `Y`:"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"zlACC1Bs6rPb","tags":["graded"]},"outputs":[{"name":"stdout","output_type":"stream","text":["Values of X: [230.1  44.5  17.2 151.5 180.8]\n","Values of Y: [22.1 10.4  9.3 18.5 12.9]\n"]}],"source":["# Create the required variables\n","X = np.array(adv['TV'])\n","Y = np.array(adv['Sales'])\n","\n","# print some of the values of X and Y\n","print('Values of X:', X[:5])\n","print('Values of Y:', Y[:5])"]},{"cell_type":"markdown","metadata":{"id":"gAXZupk96rPb"},"source":["<a name='2.1'></a>\n","### 2.1 - Linear Regression with `NumPy`"]},{"cell_type":"markdown","metadata":{"id":"b_AYPl166rPb"},"source":["You can use the function `np.polyfit(x, y, deg)` to fit a polynomial of degree `deg` to points $(x, y)$, minimising the sum of squared errors. You can read more in the [documentation](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). Taking `deg = 1` you can obtain the slope `m` and the intercept `b` of the linear regression line:"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Z9QzeLYQ6rPb","tags":["graded"]},"outputs":[{"name":"stdout","output_type":"stream","text":["Linear regression with NumPy. Slope: 0.04753664043301973. Intercept: 7.0325935491276965\n"]}],"source":["m_numpy, b_numpy = np.polyfit(X, Y, 1)\n","\n","print(f\"Linear regression with NumPy. Slope: {m_numpy}. Intercept: {b_numpy}\")"]},{"cell_type":"markdown","metadata":{"id":"3IGX-Aco6rPb"},"source":["*Note*: [`NumPy` documentation](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) suggests the [`Polynomial.fit` class method](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit) as recommended for new code as it is more stable numerically. But in this simple example, you can stick to the `np.polyfit` function for simplicity."]},{"cell_type":"markdown","metadata":{"id":"XQYzHp8-6rPb"},"source":["<a name='ex02'></a>\n","### Exercise 2\n","\n","Make predictions substituting the obtained slope and intercept coefficients into the equation $Y = mX + b$, given an array of $X$ values."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"j-ffDoJG6rPb","tags":["graded"]},"outputs":[],"source":["# This is organised as a function only for grading purposes.\n","def pred_numpy(m, b, X):\n","    Y = m*X + b\n","\n","    return Y"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"grGPkBDw6rPc","tags":["graded"]},"outputs":[{"name":"stdout","output_type":"stream","text":["TV marketing expenses:\n","[ 50 120 280]\n","Predictions of sales using NumPy linear regression:\n","[ 9.40942557 12.7369904  20.34285287]\n"]}],"source":["X_pred = np.array([50, 120, 280])\n","Y_pred_numpy = pred_numpy(m_numpy, b_numpy, X_pred)\n","\n","print(f\"TV marketing expenses:\\n{X_pred}\")\n","print(f\"Predictions of sales using NumPy linear regression:\\n{Y_pred_numpy}\")"]},{"cell_type":"markdown","metadata":{"id":"Ei1HFkW16rPc"},"source":["##### __Expected Output__\n","\n","```Python\n","TV marketing expenses:\n","[ 50 120 280]\n","Predictions of sales using NumPy linear regression:\n","[ 9.40942557 12.7369904  20.34285287]\n","```"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"J6hP1sOh6rPc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[92m All tests passed\n"]}],"source":["w2_unittest.test_pred_numpy(pred_numpy)"]},{"cell_type":"markdown","metadata":{"id":"ZrHhLUxx6rPc"},"source":["<a name='2.2'></a>\n","### 2.2 - Linear Regression with `Scikit-Learn`"]},{"cell_type":"markdown","metadata":{"id":"x9fsySKj6rPc"},"source":["`Scikit-Learn` is an open-source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. `Scikit-learn` provides dozens of built-in machine learning algorithms and models, called **estimators**. Each estimator can be fitted to some data using its `fit` method. Full documentation can be found [here](https://scikit-learn.org/stable/)."]},{"cell_type":"markdown","metadata":{"id":"S4JiS30e6rPc"},"source":["Create an estimator object for a linear regression model:"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"8GFfrNtf6rPd","tags":[]},"outputs":[],"source":["lr_sklearn = LinearRegression()"]},{"cell_type":"markdown","metadata":{"id":"PZSY20Er6rPd"},"source":["The estimator can learn from data calling the `fit` function. However, trying to run the following code you will get an error, as the data needs to be reshaped into 2D array:"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"MFpbPMHO6rPd","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X array: (200,)\n","Shape of Y array: (200,)\n","Expected 2D array, got 1D array instead:\n","array=[230.1  44.5  17.2 151.5 180.8   8.7  57.5 120.2   8.6 199.8  66.1 214.7\n","  23.8  97.5 204.1 195.4  67.8 281.4  69.2 147.3 218.4 237.4  13.2 228.3\n","  62.3 262.9 142.9 240.1 248.8  70.6 292.9 112.9  97.2 265.6  95.7 290.7\n"," 266.9  74.7  43.1 228.  202.5 177.  293.6 206.9  25.1 175.1  89.7 239.9\n"," 227.2  66.9 199.8 100.4 216.4 182.6 262.7 198.9   7.3 136.2 210.8 210.7\n","  53.5 261.3 239.3 102.7 131.1  69.   31.5 139.3 237.4 216.8 199.1 109.8\n","  26.8 129.4 213.4  16.9  27.5 120.5   5.4 116.   76.4 239.8  75.3  68.4\n"," 213.5 193.2  76.3 110.7  88.3 109.8 134.3  28.6 217.7 250.9 107.4 163.3\n"," 197.6 184.9 289.7 135.2 222.4 296.4 280.2 187.9 238.2 137.9  25.   90.4\n","  13.1 255.4 225.8 241.7 175.7 209.6  78.2  75.1 139.2  76.4 125.7  19.4\n"," 141.3  18.8 224.  123.1 229.5  87.2   7.8  80.2 220.3  59.6   0.7 265.2\n","   8.4 219.8  36.9  48.3  25.6 273.7  43.  184.9  73.4 193.7 220.5 104.6\n","  96.2 140.3 240.1 243.2  38.   44.7 280.7 121.  197.6 171.3 187.8   4.1\n","  93.9 149.8  11.7 131.7 172.5  85.7 188.4 163.5 117.2 234.5  17.9 206.8\n"," 215.4 284.3  50.  164.5  19.6 168.4 222.4 276.9 248.4 170.2 276.7 165.6\n"," 156.6 218.5  56.2 287.6 253.8 205.  139.5 191.1 286.   18.7  39.5  75.5\n","  17.2 166.8 149.7  38.2  94.2 177.  283.6 232.1].\n","Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n"]}],"source":["print(f\"Shape of X array: {X.shape}\")\n","print(f\"Shape of Y array: {Y.shape}\")\n","\n","try:\n","    lr_sklearn.fit(X, Y)\n","except ValueError as err:\n","    print(err)"]},{"cell_type":"markdown","metadata":{"id":"sFXmM3sD6rPd"},"source":["You can increase the dimension of the array by one with `reshape` function, or there is another another way to do it:"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"HI9fOBYX6rPd","tags":["graded"]},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of new X array: (200, 1)\n","Shape of new Y array: (200, 1)\n"]}],"source":["X_sklearn = X[:, np.newaxis]\n","Y_sklearn = Y[:, np.newaxis]\n","\n","print(f\"Shape of new X array: {X_sklearn.shape}\")\n","print(f\"Shape of new Y array: {Y_sklearn.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"NNM0wy686rPd"},"source":["You have already loaded your dataset into X_sklearn and Y_sklearn\n","Step 1: Split the data into training and testing sets use train_test_split from sklearn\n","The test size shoukd be 20% of the data"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"Z1cxBgLO6rPd"},"outputs":[],"source":["\n","X_train, X_test, Y_train, Y_test = train_test_split(X_sklearn, Y_sklearn, test_size=0.2, random_state=0)\n"]},{"cell_type":"markdown","metadata":{"id":"OVdNEptq6rPn"},"source":["Step 2: Fit the linear regression model to the training data\n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"JCOjcOA06rPn"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"],"text/plain":["LinearRegression()"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["lr_sklearn.fit(X_train, Y_train)"]},{"cell_type":"markdown","metadata":{},"source":["The Out put representation above is in text/plain. Click to more options to change it to the Notebook representation if you want to."]},{"cell_type":"markdown","metadata":{"id":"peVnPMvN6rPo"},"source":["\n"," Step 3: Make predictions using the fitted model on the testing data\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"iVNo1HMJ6rPo"},"outputs":[],"source":["Y_pred = lr_sklearn.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"hudBcMsU6rPo"},"source":[" Step 4: Calculate the RMSE\n","Using sklearn.metrics - mean_squared_error"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"i1UQkkC16rPo"},"outputs":[{"name":"stdout","output_type":"stream","text":["Root Mean Square Error: 3.191579849311344\n"]}],"source":["#Insert your code here\n","rmse =  np.sqrt(mean_squared_error(Y_test, Y_pred))\n","print(\"Root Mean Square Error:\", rmse)"]},{"cell_type":"markdown","metadata":{"id":"e0ohtuuC6rPo"},"source":["TO DO Create an estimator object for Random Forest and Desision Trees and compare RSMES:"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"I9M9IOlO6rPo","outputId":"106d097a-d870-4fa2-f91f-a44d4ae23805"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  return fit_method(estimator, *args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Root Mean Square Error for Random Forest: 3.9587943094223057\n","Root Mean Square Error for Decision Tree: 4.320127312938821\n"]}],"source":["# To do\n","#Print out the rank of models From the best to the worst performing and associated RSMES\n","\n","# get the random forest regressor\n","rf = RandomForestRegressor(n_estimators=1000, random_state=0)\n","\n","# fit the model\n","rf.fit(X_train, Y_train)\n","\n","# predict the values\n","Y_pred_rf = rf.predict(X_test)\n","\n","# calculate the rmse\n","rmse_rf = np.sqrt(mean_squared_error(Y_test, Y_pred_rf))\n","\n","print(\"Root Mean Square Error for Random Forest:\", rmse_rf)\n","\n","\n","# get the decision tree regressor\n","dt = DecisionTreeRegressor(random_state=0)\n","\n","# fit the model\n","dt.fit(X_train, Y_train)\n","\n","# predict the values\n","Y_pred_dt = dt.predict(X_test)\n","\n","# calculate the rmse\n","rmse_dt = np.sqrt(mean_squared_error(Y_test, Y_pred_dt))\n","\n","print(\"Root Mean Square Error for Decision Tree:\", rmse_dt)\n"]},{"cell_type":"markdown","metadata":{"id":"A7IJ43mE6rPp"},"source":["The estimator can learn from data calling the `fit` function for RandomForest and Decision Trees"]},{"cell_type":"markdown","metadata":{"id":"lk5GFVpP6rPp"},"source":["Compare the RSME for the three different models and rank them according to performance i.e Print out Model Rank and Associated RSME"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"oezuiUKg6rPp"},"outputs":[{"name":"stdout","output_type":"stream","text":["Rank of models from best to worst:\n","Linear Regression: 3.191579849311344\n","Random Forest: 3.9587943094223057\n","Decision Tree: 4.320127312938821\n"]}],"source":["# compare the ranks of the models\n","\n","# Create a dictionary of models and their RMSEs\n","models = {\n","    'Linear Regression': rmse,\n","    'Decision Tree': rmse_dt,\n","    'Random Forest': rmse_rf\n","}\n","\n","# Sort the models by their RMSEs\n","sorted_models = sorted(models.items(), key=lambda x: x[1])\n","\n","print(\"Rank of models from best to worst:\")\n","# Print the models and their RMSEs\n","for model, rmse in sorted_models:\n","    print(f\"{model}: {rmse}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["PS: Avoid running the above code without first rerunning the one that contains **rmse calculations** to ensure that the rmse value is the correct one."]},{"cell_type":"markdown","metadata":{"id":"3Dx_XduU6rPq"},"source":["<a name='ex03'></a>\n","### Exercise 3\n","\n","Fit the linear regression model passing `X_sklearn` and `Y_sklearn` arrays into the function `lr_sklearn.fit`."]},{"cell_type":"code","execution_count":59,"metadata":{"id":"obCu3yOZ6rPq","tags":["graded"]},"outputs":[{"data":{"text/html":["<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"],"text/plain":["LinearRegression()"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["### START CODE HERE ### (~ 1 line of code)\n","lr_sklearn.fit(X_sklearn, Y_sklearn)\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"LYvHrcJq6rPq","tags":["graded"]},"outputs":[{"name":"stdout","output_type":"stream","text":["Linear regression using Scikit-Learn. Slope: [[0.04753664]]. Intercept: [7.03259355]\n"]}],"source":["m_sklearn = lr_sklearn.coef_\n","b_sklearn = lr_sklearn.intercept_\n","\n","print(f\"Linear regression using Scikit-Learn. Slope: {m_sklearn}. Intercept: {b_sklearn}\")"]},{"cell_type":"markdown","metadata":{"id":"7rx3XwED6rPq"},"source":["##### __Expected Output__\n","\n","```Python\n","Linear regression using Scikit-Learn. Slope: [[0.04753664]]. Intercept: [7.03259355]\n","```"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"RxVh7BKa6rPq"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[92m All tests passed\n"]}],"source":["w2_unittest.test_sklearn_fit(lr_sklearn)"]},{"cell_type":"markdown","metadata":{"id":"XbJwE8WC6rPr"},"source":["Note that you have got the same result as with the `NumPy` function `polyfit`. Now, to make predictions it is convenient to use `Scikit-Learn` function `predict`."]},{"cell_type":"markdown","metadata":{"id":"VIHSnBHU6rPr"},"source":["<a name='ex04'></a>\n","### Exercise 4\n","\n","\n","Increase the dimension of the $X$ array using the function `np.newaxis` (see an example above) and pass the result to the `lr_sklearn.predict` function to make predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vz1rPBC76rPr","tags":[]},"outputs":[],"source":["# This is organised as a function only for grading purposes.\n","def pred_sklearn(X, lr_sklearn):\n","    ### START CODE HERE ### (~ 2 lines of code)\n","    X_2D = None[None, None.None]\n","    Y = None.None(None)\n","    ### END CODE HERE ###\n","\n","    return Y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_v7k5vNT6rPr","tags":[]},"outputs":[],"source":["Y_pred_sklearn = pred_sklearn(X_pred, lr_sklearn)\n","\n","print(f\"TV marketing expenses:\\n{X_pred}\")\n","print(f\"Predictions of sales using Scikit_Learn linear regression:\\n{Y_pred_sklearn.T}\")"]},{"cell_type":"markdown","metadata":{"id":"3quhRjne6rPr"},"source":["##### __Expected Output__\n","\n","```Python\n","TV marketing expenses:\n","[ 50 120 280]\n","Predictions of sales using Scikit_Learn linear regression:\n","[[ 9.40942557 12.7369904  20.34285287]]\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QUStlP1c6rP8"},"outputs":[],"source":["w2_unittest.test_sklearn_predict(pred_sklearn, lr_sklearn)"]},{"cell_type":"markdown","metadata":{"id":"59GjuPBy6rP8"},"source":["You can plot the linear regression line and the predictions by running the following code. The regression line is red and the predicted points are blue."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rges5Gh26rP8","tags":["graded"]},"outputs":[],"source":["fig, ax = plt.subplots(1,1,figsize=(8,5))\n","ax.plot(X, Y, 'o', color='black')\n","ax.set_xlabel('TV')\n","ax.set_ylabel('Sales')\n","\n","ax.plot(X, m_sklearn[0][0]*X+b_sklearn[0], color='red')\n","ax.plot(X_pred, Y_pred_sklearn, 'o', color='blue')"]},{"cell_type":"markdown","metadata":{"id":"czJF_mst6rP9"},"source":["<a name='3'></a>\n","## 3 - Linear Regression using Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"ZWUlaAA36rP9"},"source":["Functions to fit the models automatically are convenient to use, but for an in-depth understanding of the model and the maths behind it is good to implement an algorithm by yourself. Let's try to find linear regression coefficients $m$ and $b$, by minimising the difference between original values $y^{(i)}$ and predicted values $\\hat{y}^{(i)}$ with the **loss function** $L\\left(w, b\\right)  = \\frac{1}{2}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$ for each of the training examples. Division by $2$ is taken just for scaling purposes, you will see the reason below, calculating partial derivatives.\n","\n","To compare the resulting vector of the predictions $\\hat{Y}$ with the vector $Y$ of original values $y^{(i)}$, you can take an average of the loss function values for each of the training examples:\n","\n","$$E\\left(m, b\\right) = \\frac{1}{2n}\\sum_{i=1}^{n} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 =\n","\\frac{1}{2n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right)^2,\\tag{1}$$\n","\n","where $n$ is a number of data points. This function is called the sum of squares **cost function**. To use gradient descent algorithm, calculate partial derivatives as:\n","\n","\\begin{align}\n","\\frac{\\partial E }{ \\partial m } &=\n","\\frac{1}{n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right)x^{(i)},\\\\\n","\\frac{\\partial E }{ \\partial b } &=\n","\\frac{1}{n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right),\n","\\tag{2}\\end{align}\n","\n","and update the parameters iteratively using the expressions\n","\n","\\begin{align}\n","m &= m - \\alpha \\frac{\\partial E }{ \\partial m },\\\\\n","b &= b - \\alpha \\frac{\\partial E }{ \\partial b },\n","\\tag{3}\\end{align}\n","\n","where $\\alpha$ is the learning rate."]},{"cell_type":"markdown","metadata":{"id":"zCHTgH8t6rP9"},"source":["Original arrays `X` and `Y` have different units. To make gradient descent algorithm efficient, you need to bring them to the same units. A common approach to it is called **normalization**: substract the mean value of the array from each of the elements in the array and divide them by standard deviation (a statistical measure of the amount of dispersion of a set of values). If you are not familiar with mean and standard deviation, do not worry about this for now - this is covered in the next Course of Specialization.\n","\n","Normalization is not compulsory - gradient descent would work without it. But due to different units of `X` and `Y`, the cost function will be much steeper. Then you would need to take a significantly smaller learning rate $\\alpha$, and the algorithm will require thousands of iterations to converge instead of a few dozens. Normalization helps to increase the efficiency of the gradient descent algorithm.\n","\n","Normalization is implemented in the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NNim33L6rP9","tags":["graded"]},"outputs":[],"source":["X_norm = (X - np.mean(X))/np.std(X)\n","Y_norm = (Y - np.mean(Y))/np.std(Y)"]},{"cell_type":"markdown","metadata":{"id":"kCBxhhum6rP9"},"source":["Define cost function according to the equation $(1)$:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQ3uz1D56rP9","tags":["graded"]},"outputs":[],"source":["def E(m, b, X, Y):\n","    return #TO DO"]},{"cell_type":"markdown","metadata":{"id":"wAvTn3LN6rP9"},"source":["<a name='ex05'></a>\n","### Exercise 5\n","\n","\n","Define functions `dEdm` and `dEdb` to calculate partial derivatives according to the equations $(2)$. This can be done using vector form of the input data `X` and `Y`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uj4u10Et6rP-","tags":["graded"]},"outputs":[],"source":["def dEdm(m, b, X, Y):\n","    ### START CODE HERE ### (~ 1 line of code)\n","    # Use the following line as a hint, replacing all None.\n","    res = None\n","    ### END CODE HERE ###\n","\n","    return res\n","\n","\n","def dEdb(m, b, X, Y):\n","    ### START CODE HERE ### (~ 1 line of code)\n","    # Replace None writing the required expression fully.\n","    res = None\n","    ### END CODE HERE ###\n","\n","    return res\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYaPVRCY6rP-","tags":["graded"]},"outputs":[],"source":["print(dEdm(0, 0, X_norm, Y_norm))\n","print(dEdb(0, 0, X_norm, Y_norm))\n","print(dEdm(1, 5, X_norm, Y_norm))\n","print(dEdb(1, 5, X_norm, Y_norm))"]},{"cell_type":"markdown","metadata":{"id":"-wvF75fE6rP-"},"source":["##### __Expected Output__\n","\n","```Python\n","-0.7822244248616067\n","5.098005351200641e-16\n","0.21777557513839355\n","5.000000000000002\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mn6A-bDI6rP-"},"outputs":[],"source":["w2_unittest.test_partial_derivatives(dEdm, dEdb, X_norm, Y_norm)"]},{"cell_type":"markdown","metadata":{"id":"kbAzYiHz6rP-"},"source":["<a name='ex06'></a>\n","### Exercise 6\n","\n","\n","Implement gradient descent using expressions $(3)$:\n","\\begin{align}\n","m &= m - \\alpha \\frac{\\partial E }{ \\partial m },\\\\\n","b &= b - \\alpha \\frac{\\partial E }{ \\partial b },\n","\\end{align}\n","\n","where $\\alpha$ is the `learning_rate`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXYv_2l_6rP-","tags":["graded"]},"outputs":[],"source":["def gradient_descent(dEdm, dEdb, m, b, X, Y, learning_rate = 0.001, num_iterations = 1000, print_cost=False):\n","    for iteration in range(num_iterations):\n","        ### START CODE HERE ### (~ 2 lines of code)\n","        m_new = None\n","        b_new = None\n","        ### END CODE HERE ###\n","        m = m_new\n","        b = b_new\n","        if print_cost:\n","            print (f\"Cost after iteration {iteration}: {E(m, b, X, Y)}\")\n","\n","    return m, b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfpdIElY6rP-","tags":["graded"]},"outputs":[],"source":["print(gradient_descent(dEdm, dEdb, 0, 0, X_norm, Y_norm))\n","print(gradient_descent(dEdm, dEdb, 1, 5, X_norm, Y_norm, learning_rate = 0.01, num_iterations = 10))"]},{"cell_type":"markdown","metadata":{"id":"NmAdIeun6rP_"},"source":["##### __Expected Output__\n","\n","```Python\n","(0.49460408269589495, -3.489285249624889e-16)\n","(0.9791767513915026, 4.521910375044022)\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bsqAoog36rP_"},"outputs":[],"source":["w2_unittest.test_gradient_descent(gradient_descent, dEdm, dEdb, X_norm, Y_norm)"]},{"cell_type":"markdown","metadata":{"id":"9WQKNwAy6rP_"},"source":["Now run the gradient descent method starting from the initial point $\\left(m_0, b_0\\right)=\\left(0, 0\\right)$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6Sp5UuW6rP_","tags":["graded"]},"outputs":[],"source":["m_initial = 0; b_initial = 0; num_iterations = 30; learning_rate = 1.2\n","m_gd, b_gd = gradient_descent(dEdm, dEdb, m_initial, b_initial,\n","                              X_norm, Y_norm, learning_rate, num_iterations, print_cost=True)\n","\n","print(f\"Gradient descent result: m_min, b_min = {m_gd}, {b_gd}\")"]},{"cell_type":"markdown","metadata":{"id":"GTMtJfMH6rP_"},"source":["Remember, that the initial datasets were normalized. To make the predictions, you need to normalize `X_pred` array, calculate `Y_pred` with the linear regression coefficients `m_gd`, `b_gd` and then **denormalize** the result (perform the reverse process of normalization):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHEZXA4A6rP_","tags":[]},"outputs":[],"source":["X_pred = np.array([50, 120, 280])\n","# Use the same mean and standard deviation of the original training array X\n","X_pred_norm = (X_pred - np.mean(X))/np.std(X)\n","Y_pred_gd_norm = m_gd * X_pred_norm + b_gd\n","# Use the same mean and standard deviation of the original training array Y\n","Y_pred_gd = Y_pred_gd_norm * np.std(Y) + np.mean(Y)\n","\n","print(f\"TV marketing expenses:\\n{X_pred}\")\n","print(f\"Predictions of sales using Scikit_Learn linear regression:\\n{Y_pred_sklearn.T}\")\n","print(f\"Predictions of sales using Gradient Descent:\\n{Y_pred_gd}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HzolhFX38xyf"},"outputs":[],"source":["#What imports do we need for Fast api"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L9mgBQo67mUR"},"outputs":[],"source":["#insert fast api decorator\n","\n","def predict_fast_api(Model, tv):\n","  #Make a call to our bes model\n","\n","  tv_sales = None\n","  return tv_sales"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YR-2X9if8OQT"},"outputs":[],"source":["uvicorn.run(app, host=host, port = port)"]},{"cell_type":"markdown","metadata":{"id":"epAzrEB46rP_"},"source":["You should have gotten similar results as in the previous sections.\n","\n","Well done! Now you know how gradient descent algorithm can be applied to train a real model. Re-producing results manually for a simple case should give you extra confidence that you understand what happends under the hood of commonly used functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fB7MEAYT6rP_","tags":["graded"]},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"coursera":{"schema_names":["AI4MC1-1"]},"grader_version":"1","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"vscode":{"interpreter":{"hash":"478841ab876a4250505273c8a697bbc1b6b194054b009c227dc606f17fb56272"}}},"nbformat":4,"nbformat_minor":0}
